Spatial Analysis of Environmental Factors Related to Tornado Outbreaks
======================================================================

Research for Submission to: **Electronic Journal of Severe Storms Meteorology**

Q: What are the broad geographic variations in CAPE and shear associated with tornado outbreaks? How widely do the environmental characteristics of outbreaks vary geographically?

Q: Which environmental variable (CAPE or shear) best explains outbreak-level average tornado energy? Which variable best explains worst-case outbreak-level tornado energy?

Why is this important? Outbreaks represent the most concentrated large-scale risk for damage and casualties.

This analysis is done at the scale of tornado groups rather than at regional or local scales. 

To minimize controvery related to terminology tornado 'group' is used to refer to tornadoes occurring close together in space and time. The word 'outbreak' has a specific definition that is not strictly adhered to here. 

This code uses the Storm Prediction Center's tornado data set.

Set working directory and load packages.
```{r}
library(tidyverse)
library(lubridate)
library(sf)
library(tmap)
library(USAboundaries)
```

Download the tornado data. Source: Storm Prediction Center (SPC) http://www.spc.noaa.gov/gis/svrgis/. 
```{r, eval=TRUE}
if(!file.exists("torn")){ 
  download.file("http://www.spc.noaa.gov/gis/svrgis/zipped/tornado.zip",
                "tornado.zip", mode = "wb")
  unzip("tornado.zip")
  }
```

Read the tornado data. Add a data/time column also add columns for path length, width, and area in metric units. Remove tornadoes in Hawaii, Alaska, and Puerto Rico. Leave the time zone as native CDT. Create a convective day (6AM to 6AM) column taking hours 00:00:00 -> 05:59:59 and assigning it to the previous date (this associates the previous day's date to tornadoes occurring up to 6 hours after local midnight). The `eDate` is the date needed to download 00Z NARR data for the convective day.
```{r}
Tor.sfdf <- sf::st_read(dsn = "torn", 
                      layer = "torn", 
                      stringsAsFactors = FALSE)

Tor.sfdf <- Tor.sfdf %>%
  filter(yr >= 1994, #start in 1994 because of Radar implementation
         yr <= 2014, #NARR stops at October 2014
         mag != -9, #remove missing data
         st != c("AK", "PR", "HI")) #remove Alaska, Puerto Rico, Hawaii

Tor.sfdf <- Tor.sfdf %>%
  mutate(dy = format(as.Date(date,format="%m/%d/%y"), "%d"),
         DateTime = as.POSIXct(paste(yr, mo, dy, time), format = "%Y%m%d%H:%M:%S"),
         Hour = hour(DateTime),
         Year = year(DateTime),
         cDateTime = DateTime - as.difftime(6, unit = "hours"),
         test = as_datetime(ifelse(Hour<6, (DateTime - 86400), cDateTime), tz = Sys.timezone()),
         cDate = as.Date(test),
         eDate = as.Date(cDate) + 1, # next day for 00z data
         Length = len * 1609.34,
         Length = ifelse(Length == 0, min(Length[Length > 0]), Length), #takes care of zero length
         Width = wid * .9144,
         Width = ifelse(Width == 0, min(Width[Width > 0]), Width), #takes care of zero width
         Width = ifelse(Year >= 1995, Width * pi/4, Width), #takes care of change: avg to max
         cas = inj + fat,
         AreaPath = Length * Width,
         Ma = factor(month.abb[mo], levels = month.abb[1:12])) %>%
  sf::st_sf()
max(Tor.sfdf$yr)
```

The geometry type is `LINESTRING`. Each tornado track is a single straight line. 

Add energy dissipation per tornado. Use the empirical model for tornado winds by EF rating taken from Table 3-1 of NRC 2007. Percent area by EF rating for each EF category. Threshold wind speeds (m/s) are a lower bound 3 sec gusts on the operational EF Scale (Table 2-1 of NRC2007). This is based on work by Fricker et al.

$E = A_p \rho \sum_{j=0}^{J} w_j v_j^{3},$ where $A_p$ is the area of the path, $\rho$ is area density [1 kg/m^3]  $v_j$ is the midpoint wind speed for each rating, and $w_j$ is the corresponding fraction of path area by EF rating. With no upper bound on the EF5 wind speeds, the midpoint wind speed is set at 97 m~s$^{-1}$ (7.5 m~s$^{-1}$ above the threshold wind speed consistent with the EF4 midpoint speed relative to its threshold)
```{r}
perc <- c(1, 0, 0, 0, 0, 0, 
         .772, .228, 0, 0, 0, 0,
         .616, .268, .115, 0, 0, 0,
         .529, .271, .133, .067, 0, 0,
         .543, .238, .131, .056, .032, 0,
         .538, .223, .119, .07, .033, .017)
percM <- matrix(perc, ncol = 6, byrow = TRUE)
threshW <- c(29.06, 38.45, 49.62, 60.8, 74.21, 89.41)
midptW <- c(diff(threshW)/2 + threshW[-length(threshW)], threshW[length(threshW)] + 7.5)
ef <- Tor.sfdf$mag + 1
EW3 <- numeric()
for(i in 1:length(ef)) EW3[i] = midptW^3 %*% percM[ef[i], ]
Tor.sfdf <- Tor.sfdf %>%
  mutate(ED = EW3 * AreaPath)
```

Determine the centroid point along each track. We create a simple feature object from the `st_centroid()` function. 
Remove empty geometries using the `which()` and `st_is_empty` functions. If these are not removed, the `st_centroid()` function will not work. 
```{r}
#Tor.sfdf <- st_sf(Tor.sfdf)
#Tor.sfdf <- Tor.sfdf[-which(st_is_empty(Tor.sfdf)),]

TorP.sfdf <- st_centroid(Tor.sfdf)
TorP.sfdf$geometry[1]
```

Everything is the same except the geometry type is `POINT`. This is reflected in the column `geometry`.

Determine tornadoes that are close to each other in space and time. Start by extracting the coordinates of the centroids as a N by 2 matrix of numbers, where N is the number of tornadoes. Also extract the date-time as a vector of class `POSIXct`.
```{r}
space <- st_coordinates(TorP.sfdf)
time <- Tor.sfdf$DateTime
```

Next compute pairwise Euclidean distances in space and, separately, in time using the `dist()` function. Divide the spatial distance by 10 so that the values are comparable with the time 'distance' based on the assumption of 10 meters every second for an average speed of tornado-generating storms. 

For comparison: Distance from New York to Denver is 2.622 x 10^6 meters. There are 3.154 x 10^7 seconds in a year. This will capture the historic multiday events. For analysis we might want to consider each day in the multiday outbreak separately. As the value of the divisor increases cluster areas get larger.
```{r}
ds <- dist(space) / 10
dt <- dist(time)
dst <- ds + dt
```

Distances are saved as an object of class `dist` containing a vector of length `r N * (N-1)/2`, which is the number of unique point pairs.

Next group the tornadoes based on the space-time distances. This is done with the `hclust()` (hierarchical cluster) function. Initially, each tornado is assigned to its own group and then the algorithm joins the two closest tornadoes determined by values in `dst`. The algorithm continues by joining tornadoes (and tornado groups) until there is a single large group.

The single linkage method (`method = "single"`) is related to the minimal spanning tree (MST) and adopts a 'friends of friends' grouping strategy. An edge-weighted graph is a graph where each edge has a weight (or cost). Here weights are space-time distances between tornadoes. A MST of an edge-weighted graph is a spanning tree whose weight (the sum of the weights of its edges) is no larger than the weight of any other spanning tree. A spanning tree of a graph on N vertices (tornado centroids) is a subset of N-1 edges that form a tree (Skiena 1990, p. 227).

To extract a group number we use the `cutree()` function that cuts the tree into tornado groups where the tornadoes in each group are close in space & time. Here the tree is cut at a height of 100000 space-time units. Making `h` smaller results in smaller groups (fewer tornadoes per outbreak).
```{r}
stime <- proc.time()
tree <- hclust(dst, method = "single")
outbreakNumber <- as.integer(cutree(tree, h = 100000))
proc.time() - stime
```

  user  system elapsed 
 15.852   1.193  17.186 

Compare with DBSCAN algorithm.
```{r, eval=FALSE}
library(dbscan)
stime <- proc.time()
outbreakNumber2 <- dbscan(dst, eps = 100000, minPts = 1)$cluster
proc.time() - stime
```

   user  system elapsed 
119.500  42.834 173.344 

Add the outbreak number to each tornado then create a new simple feature object that summarizes the tornadoes by outbreak. Create unique ID for each outbreak and outbreak day. To do this combine the outbreak number and convective day with no spaces or symbols. (EX: outbreaknumber: 276; cDate: 1994-03-01 --> ID: 27619940301) 

The summary on the geometry column results in MULTILINESTRING (one line for each tornado). Keep only outbreaks with at least 30 tornadoes.
```{r}
Tor.sfdf$outbreakNumber <- outbreakNumber
Tor.sfdf <- Tor.sfdf %>%
  mutate(ID = paste0(outbreakNumber, gsub("-", "",cDate)))

Tor.sfdf$ID2 <- paste0(outbreakNumber, "-", Tor.sfdf$date)

Out.sfdf <- Tor.sfdf %>%
  group_by(outbreakNumber) %>%
  summarize(Year = first(Year),
            Month = first(mo),
            FirstDate = first(date),
            LastDate = last(date),
            Name = paste(FirstDate, "to", LastDate),
            FirstcDate = first(cDate),
            LastcDate = last(cDate),
            ncD = n_distinct(cDate),
            nT = n(),
            n0 = sum(mag == 0),
            n1 = sum(mag == 1),
            n2 = sum(mag == 2),
            n3 = sum(mag == 3),
            n4 = sum(mag == 4),
            n5 = sum(mag == 5),
            OutbreakTotalED = sum(ED),
            Name2 = paste(round(OutbreakTotalED/10^12), "TW"),
            maxEF = max(mag),
            nD = n_distinct(date),
            StartTime = first(DateTime),
            EndTime = last(DateTime),
            Duration = difftime(EndTime, StartTime, units = "secs")) %>%
  filter(nT >= 30)
Out.sfdf$TotalPathLength <- st_length(Out.sfdf)
```

Make some tables: Top 10 by number of tornadoes, energy dissipation, and duration.
```{r}
Out.sfdf %>%
  top_n(n = 10, wt = nT) %>%
  arrange(desc(nT))

Out.sfdf %>%
  top_n(n = 10, wt = OutbreakTotalED) %>%
  arrange(desc(OutbreakTotalED))

Out.sfdf %>%
  top_n(n = 10, wt = Duration) %>%
  arrange(desc(Duration))

Out.sfdf %>%
  top_n(n = 10, wt = nD) %>%
  arrange(desc(nD))
```

Map the tracks of the top 10 outbreaks by energy dissipation. First select the top 12 outbreaks. Then create a simple features data frame using only the tornadoes in this outbreak by filtering on outbreak number.
```{r}
df <- as.data.frame(Out.sfdf) %>%
  top_n(n = 12, wt = OutbreakTotalED)

sfdf <- Tor.sfdf %>%
   filter(outbreakNumber %in% df$outbreakNumber) %>%
   mutate(EF = factor(mag))

sfdf2 <- left_join(sfdf, df, by = "outbreakNumber") %>%
  arrange(desc(OutbreakTotalED))
```

Get state borders and use the `tm_shape()` function.
```{r}
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)

tm_shape(sfdf2) +
  tm_facets(by = "Name", ncol = 3)  +
  tm_lines(col = "EF", lwd = 4, palette = "Reds", n = 6) +
  tm_credits(text = df$Name2, position = c("left", "bottom"), bg.color = "white") +
tm_shape(stateBorders) + 
  tm_borders(col = "grey") +
#  tm_compass() + tm_scale_bar() +
  tm_layout(legend.outside = TRUE)
```

Distribution of the number of days.
```{r}
dfnD <- as.data.frame(Out.sfdf) %>%
  arrange(desc(nD))

table(dfnD$nD)
```

Frequency by month of the start day of multi-day outbreaks of at least 4, 5, and 6 days in length.
```{r}
as.data.frame(Out.sfdf) %>%
  filter(nD >= 4) %>%
  count(Month) %>%
  mutate(prop = prop.table(n))

as.data.frame(Out.sfdf) %>%
  count(Month, nD) %>%
  mutate(prop = prop.table(n))

```

Outbreak days. Filter individual tornadoes to remove tornadoes not in large outbreaks. Group by outbreak number and convective dates. Remove outbreak days with fewer than 10 tornadoes.
```{r}
OutDays.sfdf <- Tor.sfdf %>%
  filter(outbreakNumber %in% Out.sfdf$outbreakNumber) %>%
  group_by(outbreakNumber, cDate) %>%
  summarize(nT = n(),
            OutbreakDayTotalED = sum(ED),
            OutbreakDayMaxED = max(ED),
            OutbreakDayMeanED = mean(ED),
            eDate = first(eDate)) %>%
  filter(nT >= 10) %>%
  mutate(Year = year(eDate),
         Mo = month(eDate),
         Month = format(eDate, "%m"), # this is needed to preserve the leading zeros
         Day = format(eDate, "%d"))
```

Table the number of days per outbreak. Also look at the day counts by year and month.
```{r}
table(table(OutDays.sfdf$outbreakNumber))

table(OutDays.sfdf$Year)
table(OutDays.sfdf$Month)
```

Individual tornado days within outbreaks.
```{r}
OutDays.sfdf %>%
  top_n(n = 10, nT) %>%
  arrange(desc(nT))
```

Plot the centroid for each outbreak day and size the aesthetic by the number of tornadoes.
```{r}
outbreakDayCentroids <- st_centroid(OutDays.sfdf)
outbreakDayCentroids$nT <- OutDays.sfdf$nT
outbreakDayCentroids <- as.data.frame(outbreakDayCentroids)
outbreakDayCentroids <- as(st_sf(outbreakDayCentroids), "Spatial")

tm_shape(outbreakDayCentroids) +
  tm_symbols(size = "nT", title.size = "Count", legend.size.is.portrait = TRUE) + 
tm_shape(stateBorders) + 
  tm_borders() +
  tm_compass() + tm_scale_bar() +
  tm_layout(legend.bg.color = "white", legend.text.size = .75)
```

Outbreak day centers do not have an obvious population bias.

Estimate various spatial statistics on the outbreak days.
```{r}
library(spatstat)
library(rgeos)
library(maptools)

n <- dim(OutDays.sfdf)[1] # number of outbreak days
```

Create an ID column for the for loop so that you do not repeat convective days. If you looped through outbreak days, there are several outbreaks with tornadoes on the same outbreak day. So a unique identifier is needed with the outbreak number and the convective day. 
```{r}
OutDays.sfdf <- OutDays.sfdf %>%
  mutate(ID = paste0(outbreakNumber, gsub("-", "",cDate)))
```

Loop over all outbreak days.  See Rspatialcourse_CMIS_PDF Standard.pdf for interpretations of kappa, mu, and sigma. Kappa is the mean intensity of the inhomogeneous parent process and mu is the mean number of offsprings.
```{r}
Area <- numeric()
Intensity <- numeric()
Kappa <- numeric()
Scale <- numeric()
Mu <- numeric()

IDs <- OutDays.sfdf$ID

for(i in IDs){
 print(i)
 sfdf <- Tor.sfdf %>%
   filter(ID == i)
 sldf <- as(sfdf, "Spatial")
 hull <- gConvexHull(sldf)
 
 Area <- c(Area, gArea(hull)/10^6)  # Outbreak day area in square kilometers.
 
 T.psp <- as.psp(sldf)
 W <- as.owin(hull)
 T.psp <- T.psp[W]
 T.ppp <- endpoints.psp(T.psp, which = "first")
 T.ppp2 = rescale(T.ppp, s = 1000, unitname = "km")
 
 model <- kppm(T.ppp2, trend = ~ 1, clusters = "Thomas")
 Mu <- c(Mu, summary(model)$mu)
 Kappa <- c(Kappa, summary(model)$clustpar[1])
 Scale <- c(Scale, summary(model)$clustpar[2])
 Intensity <- c(Intensity, intensity(T.ppp2))
}

OutDays.sfdf$Area <- Area
OutDays.sfdf$Intensity <- Intensity
OutDays.sfdf$Mu <- Mu
OutDays.sfdf$Kappa <- Kappa
OutDays.sfdf$Scale <- Scale
```

Tables/graphs
```{r}
OutDays.sfdf %>%
  top_n(n = 10, "Scale") %>%
  arrange(desc(Scale))

OutDays.sfdf %>%
  group_by(Year) %>%
  summarize(avgInt = mean(Intensity),
            avgMu = mean(Mu),
            avgArea = mean(Area),
            avgKappa = mean(Kappa),
            avgScale = mean(Scale)) %>%
  ggplot(., aes(x = Year, y = avgScale)) +
    geom_point() +
    geom_smooth()
```

Get environmental data at 18Z (2p local) on the convective day. Try 00z on day after convective day (`eDate`). Also try this site https://rda.ucar.edu/datasets/ds608.0/
```{r}
library(lubridate)
df <- as.data.frame(OutDays.sfdf) %>%
  mutate(Yr = Year,
         YrMo = paste0(Year, Month),
         YrMoDa = paste0(YrMo, Day),
         slug2 = paste0(YrMo, "/", YrMoDa, "/", "narr-a_221_", YrMoDa, "_0000_000.grb"),
         slug = paste0("https://nomads.ncdc.noaa.gov/data/narr/", slug2))
```

Remove 2015 and 2016 years!
```{r}
df <- df %>%
  filter(Yr != 2015, 
         Yr != 2016,
         cDate != "2014-10-13")
```

Download grib files
```{r}
len <- dim(df)[1]
for(i in 1:len){
  download.file(df$slug[i], paste0("NARRdata", i, ".grb"), mode = "wb")
}
```

Read grib files as rasters and extract corresponding HLCY and CAPE. Start by getting the convex hull around the tracks and transforming the coordinate reference system to match that of the raster grids.
```{r}
OutDaysHulls.sfdf <- st_convex_hull(OutDays.sfdf)
OutDaysHulls.sfdf <- st_transform(OutDaysHulls.sfdf, crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs") # To match the crs of the grb rasters
```

Remove these years and cDate because no NARR data is available. 
```{r}
OutDaysHulls.sfdf <- OutDaysHulls.sfdf %>%
  filter(Year != 2015, 
         Year != 2016,
         cDate != "2014-10-13")
```

Then loop over all outbreak days extracting the average CAPE and HLCY within the hulls.
```{r}
library(raster)
aCAPE <- numeric()
aHLCY <- numeric()
aCIN <- numeric()
aLFTX <- numeric()

OutDaysHulls.sfdf <- as.data.frame(OutDaysHulls.sfdf)
OutDaysHulls.sfdf <- st_sf(OutDaysHulls.sfdf)

#Something funky with 1995-05-09 -- highest it goes to is 129...so it is changing all of the layers to 129 producing an error.

OutDaysHulls.sfdf <- OutDaysHulls.sfdf %>%
  filter(ID != "27819950509")
df <- df %>%
  filter(ID != "27819950509")

len <- dim(df)[1]

for(i in 1:len){
 print(i)
 
 rb <- brick(paste0("NARRdata", i, ".grb"))
 CAPE.rl <- raster(rb, layer = 375)
 HLCY.rl <- raster(rb, layer = 323)
 CIN.rl <- raster(rb, layer = 376)
 LFTX.rl <- raster(rb, layer = 314)
 
 aCAPE <- c(aCAPE, mean(raster::extract(CAPE.rl, OutDaysHulls.sfdf[i, ], fun = mean)))
 aHLCY <- c(aHLCY, mean(raster::extract(HLCY.rl, OutDaysHulls.sfdf[i, ], fun = mean)))
 aCIN <- c(aCIN, mean(raster::extract(CIN.rl, OutDaysHulls.sfdf[i, ], fun = mean)))
 aLFTX <- c(aLFTX, mean(raster::extract(LFTX.rl, OutDaysHulls.sfdf[i, ], fun = mean)))

}
```


Model the relationships
```{r}
len <- dim(OutDaysHulls.sfdf)[1]

sfdf <- OutDaysHulls.sfdf[1:len, ]
sfdf$aCAPE <- aCAPE
sfdf$aHLCY <- aHLCY
sfdf$aCIN <- aCIN
sfdf$aLFTX <- aLFTX
```

```{r}
sfdf %>%
  group_by(Mo) %>%
  summarize(mED = mean(OutbreakDayTotalED)/10^12,
            nT = sum(nT))
```

Models
```{r}
library(lme4)
model0 <- lmer(log(OutbreakDayTotalED) ~ scale(aHLCY) + scale(aCAPE) * scale(aCIN) + (1|Mo), data = sfdf)
summary(model0)

model0a <- lmer(log(OutbreakDayMeanED) ~ scale(aHLCY) + scale(aCAPE) * scale(aCIN) + (1|Mo), weights = nT, data = sfdf)
summary(model0a)

model1 <- lmer(log(OutbreakDayTotalED) ~ scale(aCAPE) * scale(aCIN) + (scale(aHLCY)|Mo), data = sfdf)
summary(model1)

model2 <- lmer(log(OutbreakDayMaxED) ~  scale(aHLCY) + scale(aCAPE) * scale(aCIN) + (1|Mo), data = sfdf)
summary(model2)

model4 <- lmer(log(nT) ~ scale(aHLCY) + scale(aCAPE) * scale(aCIN) + (1|Mo), data = sfdf)
summary(model4)
```

```{r}
summary(lm(log(OutbreakDayTotalED) ~ aHLCY + aCAPE + aCIN, data = sfdf[sfdf$Mo == 4, ]))
```